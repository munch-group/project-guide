{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Submodules\n",
        "\n",
        "Submodules are useful if you have analysis components that are used across many projects. It would good to have repos to use for subprojects for these components (between us, I think we have them)\n",
        "\n",
        "- HapMix\n",
        "- RFmix\n",
        "- Mosaic\n",
        "- Relate\n",
        "- ARGweaver\n",
        "- ARGweaver Clues\n",
        "- Relate Clues\n",
        "- SMC++\n",
        "- LDhat/helmet\n",
        "- pyro\n",
        "- GATK mapping and base-calling\n",
        "- ShapeIt phasing\n",
        "\n",
        "The repos can each have their own adjustable workflow as decribed [here](https://gwf.app/guide/patterns/#dynamically-generating-a-workflow), that are then easily put together in a larger workflow as described [here](https://gwf.app/guide/patterns/#large-workflows).\n",
        "\n",
        "## Preliminaries\n",
        "\n",
        "Make sure you install the current version of Git in your environments. The one on the cluster is ancient.\n",
        "\n",
        "```{.bash filename=\"Terminal\"}\n",
        "conda install -c conda-forge git\n",
        "```\n",
        "\n",
        "I also set these configs for each environment to get nicer/safer commands (commands below assume these are set):\n",
        "\n",
        "    git config --global diff.submodule log\n",
        "    git config status.submodulesummary 1\n",
        "    git config push.recurseSubmodules check\n",
        "\n",
        "Now, say you have a project repo called \"umbrella\" that will contain other projects and that you have cloned that:\n",
        "\n",
        "    git clone git@github.com:kaspermunch/umbrella.git\n",
        "\n",
        "## Add a submodule\n",
        "\n",
        "clone repository as submodule:\n",
        "\n",
        "```{.bash filename=\"Terminal\"}\n",
        "git submodule add git@github.com:munch-group/rfmix.git\n",
        "```\n",
        "and pull the current state of the submodule repo:\n",
        "\n",
        "```{.bash filename=\"Terminal\"}\n",
        "git submodule init rfmix\n",
        "git submodule update rfmix\n",
        "```\n",
        "This also generates a `.gitmodules` configuration file that git uses to keep track of submodules. Commit that the addalong with the submodule:\n",
        "\n",
        "```{.bash filename=\"Terminal\"}\n",
        "git add .gitmodules rfmix\n",
        "git commit -m 'Added rfmix as submodule'\n",
        "git push\n",
        "```\n",
        "\n",
        "If you want to work on/change submodule repo you need to check out a branch to work on (main or some other). Always do this. If you decide to make changes later and forgot you did not check out a branch you could loose those changes:\n",
        "\n",
        "````{.bash filename=\"Terminal\"}\n",
        "cd rfmix\n",
        "git checkout main  # (or some other branch)\n",
        "````\n",
        "\n",
        "## Making changes to the submodule\n",
        "\n",
        "now you can then do some work on the tester repo (E.g. change the README.md) and add, commit as usual:\n",
        "\n",
        "```{.bash filename=\"Terminal\"}\n",
        "cd rfmix\n",
        "# change README.md\n",
        "git add README.md\n",
        "git commit\n",
        "```\n",
        "\n",
        "## Publishing submodule changes to GitHub\n",
        "\n",
        "To publish your submodule commit to the tester repo on GitHub you run:\n",
        "\n",
        "```{.bash filename=\"Terminal\"}\n",
        "cd rfmix\n",
        "git push\n",
        "```\n",
        "\n",
        "## Getting submodule changes from GitHub\n",
        "\n",
        "if you run \"git pull\" in the umbrella repo, you pull upstream changes to the umbrella repo including the recorded state (commit) of the tester submodule:\n",
        "\n",
        "```{.bash filename=\"Terminal\"}\n",
        "git pull\n",
        "```\n",
        "\n",
        "but it does not pull the tester submodule itself. To do that you run pull in the submodule:\n",
        "\n",
        "```{.bash filename=\"Terminal\"}\n",
        "cd rfmix\n",
        "git pull\n",
        "```\n",
        "\n",
        "# Branches and multiple contributors\n",
        "\n",
        "As you could see above submodules are really just repos inside other repos. The parent repo just treats the submodule as a file holding which state (commit) the submodule is in. So for most use, working alone on project it is quite simple. In some cases however, you need some of the `git submodule` commands. Those commands updates the relationship between the submodule state recorded in the parent repo and the state and of the submodule.\n",
        "\n",
        "```{.bash filename=\"Terminal\"}\n",
        "git submodule update --checkout\n",
        "git submodule update --rebase\n",
        "git submodule update --merge\n",
        "```\n",
        "\n",
        "**--checkout** Checkout the commit recorded in the superproject on a detached HEAD in the submodule. This is the default behavior, the main use of this option is to override submodule.$name.update when set to a value other than checkout.\n",
        "\n",
        "**--merge** Merge the commit recorded in the superproject into the current branch of the submodule. If this option is given, the submodule’s HEAD will not be detached. If a merge failure prevents this process, you will have to resolve the resulting conflicts within the submodule with the usual conflict resolution tools. \n",
        "\n",
        "**--rebase** Rebase the current branch onto the commit recorded in the superproject. If this option is given, the submodule’s HEAD will not be detached. If a merge failure prevents this process, you will have to resolve these failures with git-rebase[1].     \n",
        "    \n",
        "```{.bash filename=\"Terminal\"}\n",
        "git submodule update --remote --checkout\n",
        "git submodule update --remote --rebase\n",
        "git submodule update --remote --merge       \n",
        "```\n",
        "\n",
        "The last one fetches upsteam changes and pulls the submoduile branch recorded in the parent repo when the submodule was added. If this branch is the current branch in the local submodule, then the command is equivalent to `git pull` in the submodule.\n",
        "\n",
        "**--remote** Instead of using the superproject’s recorded SHA-1 to update the submodule, use the status of the submodule’s remote-tracking branch. In order to ensure a current tracking branch state, update --remote fetches the submodule’s remote repository before calculating the SHA-1. If you don’t want to fetch, you should use submodule update --remote --no-fetch.\n",
        "\n",
        "Use this option to integrate changes from the upstream subproject with your submodule’s current HEAD. Alternatively, you can run git pull from the submodule, which is equivalent except for the remote branch name: update --remote uses the default upstream repository and submodule.<name>.branch, while git pull uses the submodule’s branch.<name>.merge. Prefer submodule.<name>.branch if you want to distribute the default upstream branch with the superproject and branch.<name>.merge if you want a more native feel while working in the submodule itself.\n",
        "\n",
        "(read the difference between merge and rebase [here](https://www.atlassian.com/git/tutorials/merging-vs-rebasing))\n",
        "\n",
        "\n",
        "## Multiple submodules\n",
        "\n",
        "You can have as many submodules as you want. With more submodules, each update command updates all submodules.\n",
        "\n",
        "\n",
        "# Combining GWF workflows from multiple Git submodules\n"
      ],
      "id": "fb7dfec9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import yaml\n",
        "from gwf.workflow import collect\n",
        "from gwf import Workflow, AnonymousTarget\n",
        "\n",
        "# get all targets with a given key\n",
        "def target_output_files(targets, key):\n",
        "    return [out for target in targets[key] for out in target.outputs]\n",
        "\n",
        "###############################################################################\n",
        "## Top workflow for all the stuff required to run the individual pioe lines\n",
        "###############################################################################\n",
        "\n",
        "gwf = Workflow()\n",
        "\n",
        "# read config file for workflow\n",
        "with open('workflow_config.yml') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# rfmix workflow\n",
        "from rfmix.workflow import rfmix_workflow\n",
        "\n",
        "# controls wheather submodule workflows merged with main workflow\n",
        "# or run in isolation (only use False for initial submoduile setup)\n",
        "merge_workflows = True\n",
        "\n",
        "###############################################################################\n",
        "##  Generate input files and run RFmix submodule workflow\n",
        "###############################################################################\n",
        "\n",
        "# full_list = ['Cynocephalus, Central Tanzania', 'Anubis, Kenya', 'Kindae, Zambia',\n",
        "#     'Hamadryas, Ethiopia', 'Anubis, Tanzania',\n",
        "#     'Cynocephalus, Western Tanzania', 'Papio, Senegal', 'Ursinus, Zambia',\n",
        "#     'Anubis, Ethiopia']\n",
        "\n",
        "# rfmix analyses\n",
        "rfmix_analyses = config['rfmix_analyzes']\n",
        "\n",
        "# rfmix output dir\n",
        "rfmix_output_dir = \"steps/rfmix_gen100/\"\n",
        "\n",
        "# compile reference/query sample lists for rfmix\n",
        "meta_data_samples = pd.read_csv(config['sample_meta_data'], sep=\" \")\n",
        "analyzes = []\n",
        "for analysis in rfmix_analyses:\n",
        "    d = {}\n",
        "    d[\"analysis\"] = analysis\n",
        "    os.makedirs(rfmix_output_dir+\"/\"+analysis, exist_ok=True)\n",
        "    ref_samples = meta_data_samples.loc[meta_data_samples.C_origin.isin(rfmix_analyses[analysis])]\n",
        "    query_samples = meta_data_samples.loc[~(meta_data_samples.C_origin.isin(rfmix_analyses[analysis])) &\n",
        "                                        (meta_data_samples.C_origin != \"Gelada, Captive\")]\n",
        "    d[\"ref_samples\"] =list(ref_samples.PGDP_ID)\n",
        "    d[\"query_samples\"] = list(query_samples.PGDP_ID)\n",
        "    analyzes.append(d)\n",
        "\n",
        "# write sample/population info\n",
        "for analysis in rfmix_analyses:\n",
        "    analysis_dir = rfmix_output_dir + \"/\" + analysis\n",
        "    sample_map_file = analysis_dir + \"/ref_names.txt\"\n",
        "    gwf.target(f'sample_map_{analysis}', inputs=['workflow_config.yml'], outputs=[rfmix_output_dir+\"/\"+analysis+\"/ref_names.txt\"]) << f'''\n",
        "\n",
        "    mkdir -p analysis_dir\n",
        "    python scripts/rfmix_write_sample_map.py {analysis} workflow_config.yml {sample_map_file}\n",
        "    '''\n",
        "\n",
        "# write recombination maps\n",
        "autosome_rec_map = rfmix_output_dir + \"aut_genetic_map.txt\"\n",
        "x_rec_map = rfmix_output_dir + \"X_genetic_map.txt\"\n",
        "gwf.target('format_genetic_maps', \n",
        "           memory='16gb',\n",
        "           inputs=['workflow_config.yml'], \n",
        "           outputs=[autosome_rec_map, x_rec_map]) << f'''\n",
        "\n",
        "mkdir -p output_dir\n",
        "python scripts/rfmix_format_genetic_maps.py workflow_config.yml {autosome_rec_map} {x_rec_map}\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# run the rfmix pipeline\n",
        "_gwf, rfmix_targets  = rfmix_workflow(\n",
        "                          gwf if merge_workflows else Workflow(working_dir=os.getcwd()),\n",
        "#                          merge_workflows and gwf or Workflow(working_dir=os.getcwd()),\n",
        "                          analyzes=analyzes, \n",
        "                          output_dir=rfmix_output_dir, \n",
        "                          vcf_files=config['vcf_files'], \n",
        "                          autosome_rec_map=autosome_rec_map,\n",
        "                          x_rec_map=x_rec_map\n",
        "                          )\n",
        "\n",
        "globals()['rfmix'] = _gwf\n",
        "\n",
        "###############################################################################\n",
        "## Next workflow...\n",
        "###############################################################################\n",
        "\n",
        "# # get relevant outputs from A for input to B\n",
        "# input_files =  target_output_files(rfmix_targets, 'work')\n"
      ],
      "id": "6d5de271",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In each submodule the `workflow.py` could look like this:\n",
        "\n",
        "```python\n",
        "import os.path\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from gwf import Workflow\n",
        "\n",
        "def submoduleA_workflow(working_dir=os.getcwd(), input_files=None, output_dir=None, summarize=True):\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # dict of targets as info for other workflows\n",
        "    targets = defaultdict(list)\n",
        "\n",
        "    gwf = Workflow(working_dir=working_dir)\n",
        "\n",
        "    work_output = os.path.join(output_dir, 'A_output1.txt')\n",
        "    target = gwf.target(\n",
        "        name='A_work',\n",
        "        inputs=input_files,\n",
        "        outputs=[work_output],\n",
        "    ) << f\"\"\"\n",
        "    touch {work_output}\n",
        "    \"\"\"\n",
        "    targets['work'].append(target)\n",
        "\n",
        "    if summarize:\n",
        "        summary_output = os.path.join(output_dir, 'A_output2.txt')\n",
        "        target = gwf.target(\n",
        "            name='A_summary',\n",
        "            inputs=[work_output],\n",
        "            outputs=[summary_output]\n",
        "        ) << f\"\"\"\n",
        "        touch {summary_output}\n",
        "        \"\"\"\n",
        "        targets['summary'].append(target)\n",
        "\n",
        "    return gwf, targets\n",
        "\n",
        "# we need to assign the workflow to the gwf variable to allow the workflow to be\n",
        "# run separetely with 'gwf run' in the submoduleA dir\n",
        "gwf, targets = submoduleA_workflow(input_files=['./input.txt'], output_dir='A_outputs')\n",
        "```\n",
        "\n",
        "Thw workflow can be then be run the normal way:\n",
        "\n",
        "```\n",
        "gwf run\n",
        "```\n",
        "\n",
        "This way of writing workflows allows allows multiple submodule workflows to be combined in a master workflow file.\n",
        "\n",
        "Try to put the `workflow.py` is in a `submoduleA` folder and the one below is in a `submoduleB` folder.\n",
        "\n",
        "```python\n",
        "import os.path\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from gwf import Workflow\n",
        "\n",
        "def submoduleB_workflow(working_dir=os.getcwd(), input_files=None, output_dir=None, summarize=True):\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # dict of targets as info for other workflows\n",
        "    targets = defaultdict(list)\n",
        "\n",
        "    gwf = Workflow(working_dir=working_dir)\n",
        "\n",
        "    work_output = os.path.join(output_dir, 'B_output1.txt')\n",
        "    target = gwf.target(\n",
        "        name='B_work',\n",
        "        inputs=input_files,\n",
        "        outputs=[work_output],\n",
        "    ) << f\"\"\"\n",
        "    touch {work_output}\n",
        "    \"\"\"\n",
        "    targets['work'].append(target)\n",
        "\n",
        "    if summarize:\n",
        "        summary_output = os.path.join(output_dir, 'B_output2.txt')\n",
        "        target = gwf.target(\n",
        "            name='B_summary',\n",
        "            inputs=[work_output],\n",
        "            outputs=[summary_output]\n",
        "        ) << f\"\"\"\n",
        "        touch {summary_output}\n",
        "        \"\"\"\n",
        "        targets['summary'].append(target)\n",
        "\n",
        "    return gwf, targets\n",
        "\n",
        "# we need to assign the workflow to the gwf variable to allow the workflow to be\n",
        "# run separetely with 'gwf run' in the submoduleB dir\n",
        "gwf, targets = submoduleB_workflow(input_files=['./input.txt'], output_dir='./B_outputs')\n",
        "```\n",
        "\n",
        "The two submodule workflows above can be combined into parts of a master workflow. The master `workflow.py` sits in the parent dir of `submoduleA` and `submoduleB`:\n",
        "\n",
        "```\n",
        "├── submoduleA\n",
        "│   └── workflow.py\n",
        "├── submoduleB\n",
        "│   └── workflow.py\n",
        "└── workflow.py\n",
        "```\n",
        "\n",
        "If you write it like this:\n",
        "\n",
        "```python\n",
        "import os\n",
        "from gwf.workflow import collect\n",
        "\n",
        "from submoduleA.workflow import submoduleA_workflow\n",
        "from submoduleB.workflow import submoduleB_workflow\n",
        "\n",
        "working_dir = os.getcwd()\n",
        "\n",
        "def target_output_files(targets, key):\n",
        "    return [out for target in targets[key] for out in target.outputs]\n",
        "\n",
        "# submodule A workflow\n",
        "gwf, A_targets  = submoduleA_workflow(working_dir=working_dir,\n",
        "                          input_files=['./input.txt'],\n",
        "                          output_dir='./A_outputs')\n",
        "globals()['submoduleA'] = gwf\n",
        "\n",
        "# add an extra target to glue workflows together\n",
        "gwf.target('extra', inputs=['./input.txt'], outputs=['./A_outputs/extra.txt']) << \"\"\"\n",
        "    touch ./A_outputs/extra.txt\n",
        "\"\"\"\n",
        "\n",
        "# get relevant outputs from A for input to B\n",
        "input_files =  target_output_files(A_targets, 'work')\n",
        "\n",
        "# submodule B workflow\n",
        "gwf, B_targets = submoduleB_workflow(working_dir=working_dir, \n",
        "                          input_files=input_files,\n",
        "                          output_dir='./B_outputs' )\n",
        "globals()['submoduleB'] = gwf\n",
        "```\n",
        "\n",
        "You can run each component workflow like this:\n",
        "\n",
        "```\n",
        "gwf -f workflow.py:submoduleA run\n",
        "gwf -f workflow.py:submoduleB run\n",
        "```"
      ],
      "id": "35265bb6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}